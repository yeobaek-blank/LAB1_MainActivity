{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ullOtIThwf5Y"
      },
      "source": [
        "# í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ìì—°ì–´ ëª…ë ¹\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ìì—°ì–´ ëª…ë ¹ì„ LLM(GPT-3.5 ë˜ëŠ” Gemini)ì— ì „ë‹¬í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "## âœ… ëª©í‘œ\n",
        "- ëª…ë ¹ì–´ â†’ í–‰ë™/ëŒ€ìƒ/ì¡°ê±´ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n",
        "- LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹¤í—˜\n",
        "- ê²°ê³¼ ë¶„ì„ ë° í‰ê°€"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OPENAI API**"
      ],
      "metadata": {
        "id": "UMbynCUP3JbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0Nd3iidwf5b",
        "outputId": "5a1a60bd-6eec-45e7-8527-6c271c202ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# âœ… OpenAI API ì‚¬ìš©ì„ ìœ„í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Colab í•œì •)\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° API KEY ì„¤ì •\n",
        "from openai import OpenAI # ìƒˆë¡œìš´ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ V1.0.0 ì´ìƒì—ì„œëŠ” OPENAI í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ìƒì„±í•´ì•¼ í•¨\n",
        "\n",
        "client = OpenAI(api_key='your_openai_key')  # <-- ì—¬ê¸°ì— ë³¸ì¸ì˜ OpenAI í‚¤ë¥¼ ë„£ìœ¼ì„¸ìš”"
      ],
      "metadata": {
        "id": "_yCtZRMkxBRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU2Hn-opwf5d"
      },
      "outputs": [],
      "source": [
        "# âœ… ì‹¤í—˜ìš© ëª…ë ¹ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "user_inputs = [\n",
        "    \"ê¸°ë§ê³ ì‚¬ ëŒ€ë¹„ ë„ì„œ ì•Œë ¤ì¤˜\",\n",
        "    \"KDC 600ë²ˆëŒ€ ì±… ë­ ìˆì–´?\",\n",
        "    \"ì´ë²ˆ ì£¼ ì¼ì • ì•Œë ¤ì¤˜\",\n",
        "    \"ì¸ì²œëŒ€ ë„ì„œê´€ ì—´ëŒì‹¤ í˜„í™© ì•Œë ¤ì¤˜\",\n",
        "    \"ë‚ ì”¨ê°€ ì–´ë–¤ì§€ ì•Œë ¤ì¤˜\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58MlB3gXwf5d"
      },
      "outputs": [],
      "source": [
        "# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "prompt_template = '''\n",
        "ë„ˆëŠ” ì‚¬ìš©ì ëª…ë ¹ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\n",
        "ë‹¤ìŒ ëª…ë ¹ì˜ ì˜ë¯¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë¶„í•´í•´ì¤˜.\n",
        "\n",
        "ëª…ë ¹: \"{input}\"\n",
        "\n",
        "ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\n",
        "{{\n",
        "  \"í–‰ë™\": \"ì¶”ì²œ\",\n",
        "  \"ëŒ€ìƒ\": \"ë„ì„œ\",\n",
        "  \"ì¡°ê±´\": \"ê¸°ë§ê³ ì‚¬ ëŒ€ë¹„\"\n",
        "}}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "cXO1SBfIwf5e",
        "outputId": "0a6d2ff7-d3ca-48d0-93bf-7669d31ce6a6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-1318131681.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# v1.0.0 ì´ìƒì—ì„œëŠ” í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ í†µí•´ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "# âœ… LLM í˜¸ì¶œ ë° ê²°ê³¼ ì¶œë ¥\n",
        "for input_text in user_inputs:\n",
        "    prompt = prompt_template.format(input=input_text)\n",
        "    # v1.0.0 ì´ìƒì—ì„œëŠ” í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ í†µí•´ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[{'role': 'user', 'content': prompt}]\n",
        "    )\n",
        "\n",
        "    print(f'ğŸ“Œ ëª…ë ¹ì–´: {input_text}')\n",
        "\n",
        "    print(response.choices[0].message.content)\n",
        "    print('-'*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nt7AvvFiqVHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GEMINI API**"
      ],
      "metadata": {
        "id": "3rlj8t3X3Roi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "# ğŸ”‘ API í‚¤ ë“±ë¡\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"your_gemini_key\")\n",
        "\n",
        "# ğŸ”§ ëª¨ë¸ ì„¤ì •\n",
        "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
        "\n",
        "# ğŸ§  ì‚¬ìš©ì ì…ë ¥ + í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "user_input = \"ì‹œí—˜ ì¤€ë¹„ë¥¼ ìœ„í•œ ë„ì„œë¥¼ ì¶”ì²œí•´ì¤˜\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "ë„ˆëŠ” ë„ì„œ ì¶”ì²œ ì±—ë´‡ì´ì•¼. ì‚¬ìš©ìê°€ '{user_input}'ë¼ê³  í–ˆì„ ë•Œ,\n",
        "KDC ë¶„ë¥˜ ê¸°ì¤€ì— ë”°ë¼ ì ì ˆí•œ ë„ì„œë¥¼ 3ê¶Œ ì¶”ì²œí•´ì¤˜. ì¶”ì²œ ì´ìœ ë„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜.\n",
        "\"\"\"\n",
        "\n",
        "# ğŸ§ª Gemini í˜¸ì¶œ\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# ğŸ“¤ ì‘ë‹µ ì¶œë ¥\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "SLdAmYvO3WiV",
        "outputId": "b0996496-6e42-4774-ce2d-4e1ae020fe49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 8632.28ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”! ì‹œí—˜ ì¤€ë¹„ë¥¼ ìœ„í•œ ë„ì„œë¥¼ ì°¾ìœ¼ì‹œëŠ”êµ°ìš”. KDC(í•œêµ­ì‹­ì§„ë¶„ë¥˜ë²•) ë¶„ë¥˜ ê¸°ì¤€ì— ë”°ë¼ íš¨ê³¼ì ì¸ ì‹œí—˜ ì¤€ë¹„ì— ë„ì›€ì´ ë  ë§Œí•œ ë„ì„œ 3ê¶Œì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.\n",
            "\n",
            "---\n",
            "\n",
            "**1. <ì™„ë²½í•œ ì‹œí—˜ ê³µë¶€ë²•: ë©”íƒ€ì¸ì§€ì™€ í•™ìŠµ ì „ëµ>**\n",
            "*   **KDC ë¶„ë¥˜:** 001.3 (ì§€ì‹ Â· í•™ë¬¸ Â· ì—°êµ¬ë°©ë²•ë¡  > í•™ìŠµë°©ë²•)\n",
            "*   **ì¶”ì²œ ì´ìœ :** ì´ ì±…ì€ íš¨ìœ¨ì ì¸ í•™ìŠµ ìŠµê´€ì„ í˜•ì„±í•˜ê³  ìì‹ ì—ê²Œ ë§ëŠ” ê³µë¶€ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ë©”íƒ€ì¸ì§€ë¥¼ í™œìš©í•˜ì—¬ ìì‹ ì˜ í•™ìŠµ ê³¼ì •ì„ ì ê²€í•˜ê³  ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•¨ìœ¼ë¡œì¨, ì‹œí—˜ ì¤€ë¹„ì˜ ê¸°ì´ˆë¥¼ íƒ„íƒ„íˆ ë‹¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "**2. <ì‹œí—˜ ë¶ˆì•ˆ ê·¹ë³µ: ìµœê³ ì˜ ì§‘ì¤‘ë ¥ê³¼ ë©˜íƒˆ ê´€ë¦¬>**\n",
            "*   **KDC ë¶„ë¥˜:** 159.94 (ì‹¬ë¦¬í•™ > ëŠ¥ë ¥ì‹¬ë¦¬í•™)\n",
            "*   **ì¶”ì²œ ì´ìœ :** ì‹œí—˜ ì¤€ë¹„ ê³¼ì •ê³¼ ì‹¤ì œ ì‹œí—˜ì—ì„œ ë§ˆì£¼í•  ìˆ˜ ìˆëŠ” ë¶ˆì•ˆê°ê³¼ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. ì§‘ì¤‘ë ¥ì„ ë†’ì´ê³  ê¸ì •ì ì¸ ì‚¬ê³ ë°©ì‹ì„ ìœ ì§€í•˜ëŠ” ì‹¬ë¦¬ì  ê¸°ìˆ ë“¤ì„ ì•Œë ¤ì£¼ì–´, ìì‹ ì˜ ì‹¤ë ¥ì„ ìµœëŒ€í•œ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
            "\n",
            "**3. <í•©ê²©ìœ¼ë¡œ ê°€ëŠ” ì§€ë¦„ê¸¸: ì‹œí—˜ ìœ í˜• ë¶„ì„ê³¼ ë¬¸ì œ í•´ê²° ì „ëµ>**\n",
            "*   **KDC ë¶„ë¥˜:** 371.3 (êµìœ¡í•™ > í•™ìŠµ)\n",
            "*   **ì¶”ì²œ ì´ìœ :** ë‹¨ìˆœíˆ ë‚´ìš©ì„ ì•”ê¸°í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ë‹¤ì–‘í•œ ì‹œí—˜ ìœ í˜•ì„ ë¶„ì„í•˜ê³  ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” êµ¬ì²´ì ì¸ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤. ì˜¤ë‹µ ë…¸íŠ¸ í™œìš©ë²•, ì‹œê°„ ê´€ë¦¬ ë…¸í•˜ìš° ë“±ì„ í†µí•´ ì‹¤ì „ì—ì„œ ì‘ìš©ë ¥ì„ ë†’ì´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì ìˆ˜ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤.\n",
            "\n",
            "---\n",
            "\n",
            "ì´ ë„ì„œë“¤ì´ ì‹œí—˜ ì¤€ë¹„ì— í° ë„ì›€ì´ ë˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤! ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸**"
      ],
      "metadata": {
        "id": "0OT8t5ga32vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"your_gemini_key\") # ë³¸ì¸ì˜ Gemini API í‚¤ ì‚¬ìš©\n",
        "\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "80dS1C2C3m1w",
        "outputId": "c3da83c2-f88d-4d89-e56e-48a39889c173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wscB_xvD3nIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UMbynCUP3JbT",
        "0OT8t5ga32vF"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}